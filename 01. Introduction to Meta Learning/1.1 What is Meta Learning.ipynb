{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Meta Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Learning is an exhilarating research in the field of Artificial intelligence right now. With the plenty of research papers and advancements, meta learning is clearly making a major breakthrough in AI. Before getting into meta learning, let us inspect how our current AI model works. Deep learning has progressed rapidly in recent years with great algorithms like generative adversarial networks, capsule networks etc... But the problem with deep neural networks is that we need to have large training set to train our model and it will fail abruptly when we have very less data points. Let us say you trained a deep learning model for performing some task A. Now when you have a new task B which is closely related to A, you can't use the same model. We need to train the model from scratch for task B. So for each task, we need to train the model from scratch although they might be related.\n",
    "\n",
    "Is deep learning really the true AI? Well, it is not. How do we humans learn? We generalize our learning to multiple concepts and\n",
    "learn from there. But current learning algorithms master only one task. Here is where meta learning comes in. Meta learning produces a versatile AI model that can learn to perform various tasks without having to train them from scratch. We train our meta learning model on various related tasks with few data points, so for a new related task, it can make use of the learning obtained from the previous tasks without having to train them scratch. Many researchers and scientists believe that meta learning can get us closer to achieving Artificial intelligence. We will learn how exactly meta learning models learn the learning process in the upcoming section.A Simple View
A good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on a distribution of tasks, including potentially unseen tasks. Each task is associated with a dataset D, containing both feature vectors and true labels. The optimal model parameters are:

θ∗=argminθED∼p(D)[Lθ(D)]
It looks very similar to a normal learning task, but one dataset is considered as one data sample.

Few-shot classification is an instantiation of meta-learning in the field of supervised learning. The dataset D is often split into two parts, a support set S for learning and a prediction set B for training or testing, D=⟨S,B⟩. Often we consider a K-shot N-class classification task: the support set contains K labelled examples for each of N classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
